---
title: "Impact of Storm Events on Public Health and Economics"
author: "Hugo van den Berg"
date: "August 21, 2016"
output: 
    html_document:
      toc: yes
    github_document:
      toc: yes
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = file.path(getwd(), '..'))
knitr::opts_chunk$set(echo = TRUE,
                      cache.path = 'cache')
config.base.url <- 'https://github.com/Hugovdberg/StormData/blob/master/'
```

## Synopsis

In this report I aim to identify the impact of storm events on public health
and economics from (a subset of) the [NOAA Storm Database][1], collected
between 1950 and 2011.
The dataset contains a number of variables describing discrete weather events,
on time, location, and trajectory, as well as the number of victims (direct or
indirect), and damages to properties and crops.

The analysis is divided in two parts, the first of which identifies which type
of events has caused the largest total number of victims, as well as per single
event.
The second part is focussed on the financial impact of weather events, again
in total over all recorded events, as well as per event.



## 1. Data Processing

### 1.1 Libraries

For this analysis we used a range of libraries, which are listed below with
their respective version numbers.
Note this block was weaved with the option `message=FALSE` to hide startup
messages.

```{r load_libs, message=FALSE}
# Reading data
library(readr) # v1.0.0

# Munging data
library(magrittr) # v1.5
library(tidyr) # v0.6.0
library(plyr) # v1.8.4
library(dplyr) # v0.5.0
library(lubridate) # v1.5.6
library(stringr) # v1.1.0

# Plotting data
library(ggplot2) # v2.1.0
```

### 1.2 Raw data collection

The [NOAA Storm Database][1] is publicly available but for this report make use
of a dedicated subset specially provided for the Coursera course *[Reproducible
Research][2]*.

```{r get_data, cache=TRUE}
data.url <- paste0('https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2F',
                   'StormData.csv.bz2')
data.raw <- file.path('data', 'StormData.csv.bz2')
if (!file.exists(data.raw)) {
    download.file(url = data.url, destfile = data.raw, mode = "wb")
}

data <- read_csv(file = data.raw,
                 col_types = 'dcccdcccdccccdcdccdddddddcdccccddddcc',
                 progress = FALSE)
data.backup <- data
data <- data.backup
rm(data.url, data.raw)
```

### 1.3 Cleaning the data

The raw data contains various variables that are not in optimal form to perform
further analyses.

#### Dates

Dates in the original data are scattered over a number of variables, describing
dates in various different timezones.
To create uniform dates, that can be handled consistently, all dates and times
are converted to `POSIXct` objects.

Unfortunately timezones are hard to get right, and the conversion functions
don't support reading the timezone from the inputstring.
Therefore the dataset is split along the `TIME_ZONE` variable, after which for
each split the dates are converted before merging the data back together.

```{r date_convert, cache=TRUE}
# Add relevant timezone information for date conversion
TZ <- read_csv(file.path('data', 'TZ.csv'))
data %<>% inner_join(TZ)

# Split the data along the various timezones
date.data <- split(x = data, f = data$timezone)
for (tz in seq_along(date.data)) {
    date.data[[tz]] %<>%
        # Extract only date part of begin date
        separate("BGN_DATE", "BGN_DATE_",
                 sep = " ", extra = "drop", remove = F) %>%
        # Extract only date part of end date
        separate("END_DATE", "END_DATE_",
                 sep = " ", extra = "drop", remove = F) %>%
        mutate(
            # Set begin time to 0:00 if missing
            BGN_TIME_ = ifelse(is.na(BGN_TIME),
                               '0000',
                               substr(gsub(':', '', BGN_TIME), 1, 4)),
            # Set end time to 0:00 if missing
            END_TIME_ = ifelse(is.na(END_TIME),
                               '0000',
                               substr(gsub(':', '', END_TIME), 1, 4)),
            # Convert begin date and time to POSIXct
            begin.date = as.POSIXct(paste(BGN_DATE_, BGN_TIME_),
                                    format = '%m/%d/%Y %H%M',
                                    tz = timezone[1]),
            # Convert end date and time to POSIXct
            end.date = as.POSIXct(paste(END_DATE_, END_TIME_),
                                  format = '%m/%d/%Y %H%M',
                                  tz = timezone[1])
        )
}
# Join split dataset
data <- bind_rows(date.data)
rm(date.data, tz, TZ)
```

#### Event types

The event types contain a lot of misspelled event types.
The [codebook][3] describes a list of all event types in the `EVTYPE` variable
and a mapping to more consistent names, which is stored in 
[EVTYPE.csv][4].
After renaming the event types they are split on the "/"-character into a
major and minor event type, under the assumption that the major event type is
listed first in the `EVTYPE` variable.

```{r evtype_mapping}
evtypes <- read_csv(file.path('data', 'EVTYPE.csv'), col_types = 'cc')
data %<>% 
    mutate(EVTYPE = mapvalues(EVTYPE, 
                              from = evtypes$from, 
                              to = evtypes$to)) %>%
    separate(EVTYPE, into = c("event.type.major", "event.type.minor"), 
             sep = '/', extra = 'merge', fill = "right") %>%
    mutate_at(vars(event.type.major, event.type.minor), funs(as.factor))
rm(evtypes)
```

#### States and regions

The `STATE` variable contains a number of non-standard state codes.
The [codebook][3] describes a list of all state codes and a mapping to
consistent names for the regions in which the event occurred, which is stored
in [STATE.csv][5].

```{r region_mapping}
states <- read_csv(file.path('data', 'STATE.csv'), col_types = 'cc')
data %<>% 
    mutate(
        region = as.factor(mapvalues(STATE,
                                     from = states$from,
                                     to = states$to))
        )
rm(states)
```

#### Latitude and Longitude

The `LATITUDE`, `LONGITUDE`, `LATITUDE_E`, and `LONGITUDE_` variables contain
the respective latitude and longitude of the start and end of the weather
event.
Both are given in hundreths of degrees, and converted to positive values for
the longitudes.
Missing values in the dataset are represented as `0`, these are converted to
`NA`.

```{r lat_lon}
data %<>%
    mutate(begin.latitude = ifelse(LATITUDE, LATITUDE/100, NA),
           begin.longitude = ifelse(LONGITUDE, -LONGITUDE/100, NA),
           end.latitude = ifelse(LATITUDE_E, LATITUDE_E/100, NA),
           end.longitude = ifelse(LONGITUDE_, -LONGITUDE_/100, NA))
```

#### Damages to Properties and Crops

```{r}
data %<>%
    mutate(PROPDMGEXP = ifelse(toupper(PROPDMGEXP) %in% c("K", "M", "B"),
                               toupper(PROPDMGEXP),
                               "1"),
           PROPDMGEXP = mapvalues(PROPDMGEXP,
                                  c("K",   "M",   "B"),
                                  c("1e3", "1e6", "1e9")),
           property.damages = PROPDMG * as.numeric(PROPDMGEXP),
           CROPDMGEXP = ifelse(toupper(CROPDMGEXP) %in% c("K", "M", "B"),
                               toupper(CROPDMGEXP),
                               "1"),
           CROPDMGEXP = mapvalues(CROPDMGEXP,
                                  c("K",   "M",   "B"),
                                  c("1e3", "1e6", "1e9")),
           crop.damages = CROPDMG * as.numeric(CROPDMGEXP))
```

#### Relevant variables

Now all relevant variables have been cleaned up the final selection of the
features can be made.


## 2. Results

```{r summ}
# summary(data)

```

### 2.1 Personal impact



```{r plot_personal}
top_events <- data %>%
    group_by(event.type.major) %>%
    summarise(victims = sum(FATALITIES) + sum(INJURIES)) %>%
    top_n(10, victims) %>%
    droplevels()
event_levels <- levels(top_events$event.type.major)[order(top_events$victims,
                                                          decreasing = TRUE)]
personal <- data %>%
    filter(event.type.major %in% top_events$event.type.major) %>%
    select(begin.date, event.type.major, FATALITIES, INJURIES) %>%
    mutate(
        event.type.major = ordered(event.type.major, levels = event_levels),
        fatalities = FATALITIES,
        injuries = INJURIES
        ) %>%
    gather(impact, victims, fatalities, injuries) %>%
    group_by(event.type.major, impact) %>%
    summarise(sum = sum(victims, na.rm = TRUE),
              mean = mean(victims, na.rm = TRUE)) %>%
    gather(func, victims, sum, mean) %>%
    mutate(func = factor(func, levels = c("sum", "mean"),
                         labels = c("Total", "Avg. per event")))
```

```{r}
personal %>%
    ggplot(aes(x = event.type.major, y = victims, fill = impact)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(func ~ ., scales = "free_y")  +
    # theme_economist() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    scale_y_log10() +
    scale_fill_discrete(name = "Type of victim",
                        labels = c("Fatalities", "Injuries")) +
    coord_cartesian(xlim = c(0.8, 10.2)) +
    annotation_logticks(sides = "rl", alpha = 0.3) +
    labs(title = "Top 10 weather event types, ranked by total number of victims",
         x = "Major event type",
         y = "Number of victims")
```


### 2.2 Financial impact


```{r plot_financial}
top_events <- data %>%
    group_by(event.type.major) %>%
    summarise(damages = sum(PROPDMG) + sum(INJURIES)) %>%
    top_n(10, victims) %>%
    droplevels()
event_levels <- levels(top_events$event.type.major)[order(top_events$victims,
                                                          decreasing = TRUE)]
personal <- data %>%
    filter(event.type.major %in% top_events$event.type.major) %>%
    select(begin.date, event.type.major, FATALITIES, INJURIES) %>%
    mutate(
        event.type.major = ordered(event.type.major, levels = event_levels),
        fatalities = FATALITIES,
        injuries = INJURIES
        ) %>%
    gather(impact, victims, fatalities, injuries) %>%
    group_by(event.type.major, impact) %>%
    summarise(sum = sum(victims, na.rm = TRUE),
              mean = mean(victims, na.rm = TRUE)) %>%
    gather(func, victims, sum, mean) %>%
    mutate(func = factor(func, levels = c("sum", "mean"),
                         labels = c("Total", "Avg. per event")))
```

```{r}
personal %>%
    ggplot(aes(x = event.type.major, y = victims, fill = impact)) +
    geom_bar(stat = "identity", position = "dodge") +
    facet_grid(func ~ ., scales = "free_y")  +
    # theme_economist() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    scale_y_log10() +
    scale_fill_discrete(name = "Type of victim",
                        labels = c("Fatalities", "Injuries")) +
    coord_cartesian(xlim = c(0.8, 10.2)) +
    annotation_logticks(sides = "rl", alpha = 0.3) +
    labs(title = "Top 10 weather event types, ranked by total number of victims",
         x = "Major event type",
         y = "Number of victims")
```


[1]: http://www.ncdc.noaa.gov/stormevents/ 
[2]: https://www.coursera.org/learn/reproducible-research
[3]: `r config.base.url`doc/CodeBook.md
[4]: `r config.base.url`data/EVTYPE.csv
[5]: `r config.base.url`data/STATE.csv
